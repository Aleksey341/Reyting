"""
API routes for importing CSV data
"""
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from sqlalchemy.orm import Session
from sqlalchemy import text
import logging
import pandas as pd
import io
import json

from database import get_db
from models import DimMO, DimPeriod, DimIndicator, DimMethodology, FactIndicator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.post("/official-methodology")
async def import_official_methodology_csv(
    file: UploadFile = File(...),
    period_month: str = "2024-01",
    db: Session = Depends(get_db),
):
    """
    Import CSV file with official methodology indicator data.

    Expected CSV format:
    - First column: –ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç (Municipality name)
    - Columns: pub_1, pub_2, pub_3, pub_4, pub_5, pub_6, pub_7, pub_8, pub_9
               closed_1, closed_2, closed_3, closed_4, closed_5, closed_6, closed_7, closed_8
               pen_1, pen_2, pen_3

    Example row:
    –õ–∏–ø–µ—Ü–∫, 3, 5, 3, 3, 3, 3, 3, 3, 3, 6, 4, 5, 5, 3, 3, 6, 2, -3, -3, -5

    Parameters:
    - file: CSV file to upload
    - period_month: Period in YYYY-MM format (default: 2024-01)
    """
    try:
        # Read CSV file
        content = await file.read()
        df = pd.read_csv(io.BytesIO(content), encoding='utf-8')

        logger.info(f"CSV uploaded: {len(df)} rows, {len(df.columns)} columns")
        logger.info(f"Target period: {period_month}")
        logger.info(f"Columns: {list(df.columns)}")

        # Parse period_month
        from datetime import datetime, timedelta
        period_date = datetime.strptime(period_month, "%Y-%m")

        # Calculate last day of month
        if period_date.month == 12:
            period_end = period_date.replace(day=31)
        else:
            next_month = period_date.replace(month=period_date.month + 1, day=1)
            period_end = next_month - timedelta(days=1)

        date_from = period_date.strftime("%Y-%m-%d")
        date_to = period_end.strftime("%Y-%m-%d")

        # Ensure official methodology exists
        logger.info("Ensuring official methodology exists...")
        from migrations import implement_official_methodology
        implement_official_methodology()

        # Get or create official methodology version
        methodology = db.query(DimMethodology).first()
        if not methodology:
            methodology = DimMethodology(
                version="Official v1",
                valid_from="2024-01-01",
                notes="Official methodology with 16 criteria"
            )
            db.add(methodology)
            db.commit()
            db.refresh(methodology)
            logger.info(f"Created official methodology (ID: {methodology.version_id})")

        # Get or create period
        period = db.query(DimPeriod).filter(
            DimPeriod.date_from == date_from
        ).first()

        if not period:
            period = DimPeriod(
                period_type="month",
                date_from=date_from,
                date_to=date_to,
                edg_flag=False
            )
            db.add(period)
            db.commit()
            db.refresh(period)
            logger.info(f"Created period {period_month} (ID: {period.period_id})")

        # Official indicator codes
        official_indicators = [
            'pub_1', 'pub_2', 'pub_3', 'pub_4', 'pub_5', 'pub_6', 'pub_7', 'pub_8', 'pub_9',
            'closed_1', 'closed_2', 'closed_3', 'closed_4', 'closed_5', 'closed_6', 'closed_7', 'closed_8',
            'pen_1', 'pen_2', 'pen_3'
        ]

        # Load data
        values_loaded = 0
        errors = 0
        rows_processed = 0

        for _, row in df.iterrows():
            mo_name = row.get('–ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç') or row.get('municipalitet') or row.get('mo_name')

            if pd.isna(mo_name) or mo_name == '':
                continue

            rows_processed += 1

            # Find MO by name
            mo = db.query(DimMO).filter(
                text("mo_name ILIKE :name")
            ).params(name=f"%{mo_name}%").first()

            if not mo:
                logger.warning(f"Municipality not found: {mo_name}")
                continue

            # Load official indicators
            for ind_code in official_indicators:
                # Check if column exists in CSV
                if ind_code not in row:
                    continue

                value = row[ind_code]
                if pd.isna(value) or value == '':
                    continue

                # Find indicator
                indicator = db.query(DimIndicator).filter(
                    DimIndicator.code == ind_code
                ).first()

                if not indicator:
                    logger.warning(f"Indicator not found: {ind_code}")
                    continue

                # Convert value to float
                try:
                    if isinstance(value, str):
                        value_clean = value.replace('%', '').replace(' ', '').replace(',', '.')
                        try:
                            value_float = float(value_clean)
                        except:
                            value_float = 0.0
                    else:
                        value_float = float(value)
                except:
                    value_float = 0.0

                # Check if exists
                existing = db.query(FactIndicator).filter(
                    FactIndicator.mo_id == mo.mo_id,
                    FactIndicator.period_id == period.period_id,
                    FactIndicator.ind_id == indicator.ind_id,
                    FactIndicator.version_id == methodology.version_id
                ).first()

                if existing:
                    existing.score = value_float
                else:
                    fact = FactIndicator(
                        mo_id=mo.mo_id,
                        period_id=period.period_id,
                        ind_id=indicator.ind_id,
                        version_id=methodology.version_id,
                        score=value_float,
                    )
                    db.add(fact)

                values_loaded += 1

        db.commit()
        logger.info(f"Loaded {values_loaded} indicator values from {rows_processed} rows")

        # Automatically calculate aggregated scores
        logger.info(f"Auto-calculating aggregated scores for period {period.period_id}...")
        try:
            from migrations import calculate_fact_summary_from_indicators
            calculate_fact_summary_from_indicators()
            logger.info("‚úì Aggregated scores calculated successfully")
        except Exception as e:
            logger.error(f"Error calculating aggregated scores: {e}")

        return {
            "status": "success",
            "message": "Official methodology data imported successfully!",
            "statistics": {
                "rows_processed": rows_processed,
                "values_loaded": values_loaded,
                "period": period_month,
                "period_id": period.period_id,
                "methodology": "Official 16 criteria"
            },
            "next_steps": [
                "1. Hard refresh Rating tab (Ctrl+F5)",
                "2. Scores should display with proper aggregation"
            ]
        }

    except Exception as e:
        logger.error(f"Error importing official methodology CSV: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error importing CSV: {str(e)}")


@router.post("/official-methodology-excel")
async def import_official_methodology_excel(
    file: UploadFile = File(...),
    period_month: str = "2024-01",
    db: Session = Depends(get_db),
):
    """
    Import Excel file with official methodology indicator data.

    Supports two formats:
    1. Single sheet with columns: –ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç, pub_1, pub_2, ... closed_1, ... pen_1, etc.
    2. Multiple sheets (one per criterion) with –ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç and values

    Parameters:
    - file: Excel file to upload (.xlsx)
    - period_month: Period in YYYY-MM format (default: 2024-01)
    """
    try:
        # Read Excel file
        content = await file.read()
        logger.info(f"üîπ Received file: {file.filename}, size: {len(content)} bytes")

        # Get all sheet names
        xls = pd.ExcelFile(io.BytesIO(content))
        sheet_names = xls.sheet_names
        logger.info(f"üîπ Excel file has {len(sheet_names)} sheets: {sheet_names}")

        # Debug: Return sheet info immediately
        if not sheet_names:
            logger.error("‚ùå No sheets found in Excel file!")
            return {
                "status": "error",
                "message": "Excel file has no sheets",
                "debug": {
                    "file_size": len(content),
                    "sheets_found": 0
                }
            }

        # Parse period_month
        from datetime import datetime, timedelta
        period_date = datetime.strptime(period_month, "%Y-%m")

        # Calculate last day of month
        if period_date.month == 12:
            period_end = period_date.replace(day=31)
        else:
            next_month = period_date.replace(month=period_date.month + 1, day=1)
            period_end = next_month - timedelta(days=1)

        date_from = period_date.strftime("%Y-%m-%d")
        date_to = period_end.strftime("%Y-%m-%d")

        # Ensure official methodology exists
        logger.info("Ensuring official methodology exists...")
        from migrations import implement_official_methodology
        implement_official_methodology()

        # Get or create official methodology version
        methodology = db.query(DimMethodology).first()
        if not methodology:
            methodology = DimMethodology(
                version="Official v1",
                valid_from="2024-01-01",
                notes="Official methodology with 16 criteria"
            )
            db.add(methodology)
            db.commit()
            db.refresh(methodology)
            logger.info(f"Created official methodology (ID: {methodology.version_id})")

        # Get or create period
        period = db.query(DimPeriod).filter(
            DimPeriod.date_from == date_from
        ).first()

        if not period:
            period = DimPeriod(
                period_type="month",
                date_from=date_from,
                date_to=date_to,
                edg_flag=False
            )
            db.add(period)
            db.commit()
            db.refresh(period)
            logger.info(f"Created period {period_month} (ID: {period.period_id})")

        # Official indicator codes
        official_indicators = [
            'pub_1', 'pub_2', 'pub_3', 'pub_4', 'pub_5', 'pub_6', 'pub_7', 'pub_8', 'pub_9',
            'closed_1', 'closed_2', 'closed_3', 'closed_4', 'closed_5', 'closed_6', 'closed_7', 'closed_8',
            'pen_1', 'pen_2', 'pen_3'
        ]

        # Determine format: Check if we have multiple sheets AND can identify criterion codes
        # Format 2: Multiple sheets (one per criterion) - usually 16+ sheets with criterion names
        # Format 1: Single sheet with all columns

        values_loaded = 0
        total_rows_processed = 0

        # For multi-sheet format, check if sheets match criterion names
        has_multiple_sheets = len(sheet_names) > 1

        # Try to detect if this is multi-sheet format by checking sheet names
        is_multisheet_format = False
        if has_multiple_sheets:
            # Check if sheet names contain known criterion keywords
            criterion_keywords = [
                '–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ', '–ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '–ø—Ä–æ–µ–∫—Ç–Ω–∞—è', '–º–æ–ª–æ–¥–µ–∂–∏', '–≤–µ—Ç–µ—Ä–∞–Ω',
                '–∫–∞–¥—Ä–æ–≤—ã–π', '–≥—Ä–∞–Ω—Ç—ã', '–ø–∞—Ä—Ç–∏–π–Ω–∞—è', '–º–∞–Ω–¥–∞—Ç–æ–≤', '–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏', '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫',
                '–∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã', '–ø—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö'
            ]
            matching_sheets = sum(1 for sheet in sheet_names
                                if any(kw in sheet.lower() for kw in criterion_keywords))
            is_multisheet_format = matching_sheets >= 10  # If 10+ sheets have criterion names
            logger.info(f"üîπ Multi-sheet detection: {matching_sheets}/{len(sheet_names)} sheets match criterion names")

        # **Format 2**: Multiple sheets - one per criterion
        if is_multisheet_format:
            logger.info("Detected Format 2: Multiple sheets (one per criterion)")

            # Mapping from criterion name (sheet name) to official code
            # This maps the Russian criterion names to official codes
            criterion_name_to_code = {
                # PUBLIC CRITERIA (pub_1 to pub_9)
                "–û—Ü–µ–Ω–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –æ–±": "pub_1",  # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –æ–±–ª–∞—Å—Ç–∏
                "–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á –ê–ì–ü": "pub_2",  # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á –ê–ì–ü
                "–ü–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–ª–∞–≤—ã –ú–û": "pub_3",
                "–ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å": "pub_4",
                "–í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å –º–æ–ª–æ–¥–µ–∂–∏ _–î–æ–±—Ä–æ–≤–æ": "pub_5",  # –ú–æ–ª–æ–¥–µ–∂—å –≤ –¥–æ–±—Ä–æ–≤–æ–ª—å—á–µ—Å—Ç–≤–µ
                "–í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å –º–æ–ª–æ–¥–µ–∂–∏ _–î–≤–∏–∂–µ–Ω–∏": "pub_6",  # –ú–æ–ª–æ–¥–µ–∂—å –≤ –î–≤–∏–∂–µ–Ω–∏–∏ –ü–µ—Ä–≤—ã—Ö
                "–õ–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –≥–ª–∞–≤—ã —Å –≤–µ—Ç–µ—Ä–∞–Ω–∞–º": "pub_7",  # –†–∞–±–æ—Ç–∞ —Å –≤–µ—Ç–µ—Ä–∞–Ω–∞–º–∏ –°–í–û
                "–ö–∞–¥—Ä–æ–≤—ã–π —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–π —Ä–µ–∑–µ—Ä–≤": "pub_8",
                "–†–∞–±–æ—Ç–∞ —Å –≥—Ä–∞–Ω—Ç–∞–º–∏": "pub_9",

                # CLOSED CRITERIA (closed_1 to closed_8)
                "–ü–∞—Ä—Ç–∏–π–Ω–∞—è –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å —Å–æ—Ç—Ä—É–¥": "closed_1",  # –ü–∞—Ä—Ç–∏–π–Ω–æ–µ –º–Ω–µ–Ω–∏–µ –≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏
                "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–Ω–¥–∞—Ç–æ–≤": "closed_2",  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –º–Ω–µ–Ω–∏–µ –≤ –æ—Ä–≥–∞–Ω–µ
                "–ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ê–ì–ü _–£—Ä–æ–≤–µ–Ω—å_": "closed_3",
                "–ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ê–ì–ü _–ö–∞—á–µ—Å—Ç–≤–æ_": "closed_4",
                "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å": "closed_5",
                "–õ–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –≥–ª–∞–≤—ã —Å –≤–µ—Ç–µ—Ä–∞–Ω–∞–º": "closed_6",  # –†–∞–±–æ—Ç–∞ —Å –≤–µ—Ç–µ—Ä–∞–Ω–∞–º–∏ –°–í–û (–∑–∞–∫—Ä—ã—Ç–∞—è)
                "–ü–∞—Ä—Ç–∏–π–Ω–∞—è –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –≤–µ—Ç–µ—Ä–∞": "closed_7",  # –ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–µ—Ç–µ—Ä–∞–Ω–æ–≤
                "–£—á–∞—Å—Ç–∏–µ –≤ –ø—Ä–æ–µ–∫—Ç–µ _–ì–æ—Ä–¥–æ—Å—Ç—å –õ–∏–ø": "closed_8",  # –ü—Ä–æ–µ–∫—Ç –ì–æ—Ä–¥–æ—Å—Ç—å –õ–∏–ø–µ—Ü–∫–æ–π –∑–µ–º–ª–∏

                # PENALTY CRITERIA (pen_1 to pen_3)
                "–ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã —Å —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–π –≤–ª–∞—Å—Ç—å": "pen_1",
                "–í–Ω—É—Ç—Ä–∏–º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã": "pen_2",
                "–î–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω": "pen_3",
            }

            # Build mapping from sheet name to official code
            sheet_to_code = {}
            for sheet_name in sheet_names:
                # Try exact match first
                if sheet_name in criterion_name_to_code:
                    sheet_to_code[sheet_name] = criterion_name_to_code[sheet_name]
                else:
                    # Try partial match
                    for criterion_name, code in criterion_name_to_code.items():
                        if criterion_name.lower() in sheet_name.lower() or sheet_name.lower() in criterion_name.lower():
                            sheet_to_code[sheet_name] = code
                            break

            logger.info(f"Sheet to code mapping: {sheet_to_code}")

            # Process each sheet as a separate criterion
            for sheet_name in sheet_names:
                if sheet_name not in sheet_to_code:
                    logger.info(f"Skipping sheet '{sheet_name}' - no matching criterion code")
                    continue

                indicator_code = sheet_to_code[sheet_name]
                logger.info(f"Processing sheet '{sheet_name}' as criterion '{indicator_code}'")

                df = pd.read_excel(io.BytesIO(content), sheet_name=sheet_name)
                logger.info(f"Sheet '{sheet_name}': {df.shape} - columns: {list(df.columns)}")

                # Find municipality column
                mo_col_name = None
                for col in df.columns:
                    if '–º—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç' in str(col).lower():
                        mo_col_name = col
                        break

                if not mo_col_name:
                    logger.warning(f"No municipality column found in sheet '{sheet_name}'")
                    continue

                # Find value column (usually second column or numeric column)
                value_col_name = None
                for col in df.columns:
                    if col != mo_col_name:
                        # Check if column contains numeric values
                        try:
                            sample = pd.to_numeric(df[col].dropna().head(1), errors='coerce')
                            if not sample.isna().all():
                                value_col_name = col
                                break
                        except:
                            pass

                if not value_col_name:
                    logger.warning(f"No numeric column found in sheet '{sheet_name}'")
                    continue

                logger.info(f"Using columns: MO='{mo_col_name}', Value='{value_col_name}'")

                # Get or create indicator
                indicator = db.query(DimIndicator).filter(
                    DimIndicator.code == indicator_code
                ).first()

                if not indicator:
                    logger.warning(f"Indicator {indicator_code} not found in database")
                    continue

                # Load data from this sheet
                for idx, row in df.iterrows():
                    mo_name = row.get(mo_col_name)
                    if pd.isna(mo_name) or mo_name == '':
                        continue

                    total_rows_processed += 1

                    # Find MO
                    mo = db.query(DimMO).filter(
                        text("mo_name ILIKE :name")
                    ).params(name=f"%{mo_name}%").first()

                    if not mo:
                        logger.warning(f"Municipality '{mo_name}' not found")
                        continue

                    # Get value
                    value = row.get(value_col_name)
                    if pd.isna(value) or value == '':
                        continue

                    # Convert to float
                    try:
                        if isinstance(value, str):
                            value_clean = value.replace('%', '').replace(' ', '').replace(',', '.')
                            value_float = float(value_clean)
                        else:
                            value_float = float(value)
                    except:
                        logger.warning(f"Could not convert value '{value}' to float")
                        continue

                    # Insert or update
                    existing = db.query(FactIndicator).filter(
                        FactIndicator.mo_id == mo.mo_id,
                        FactIndicator.period_id == period.period_id,
                        FactIndicator.ind_id == indicator.ind_id,
                        FactIndicator.version_id == methodology.version_id
                    ).first()

                    if existing:
                        existing.score = value_float
                    else:
                        fact = FactIndicator(
                            mo_id=mo.mo_id,
                            period_id=period.period_id,
                            ind_id=indicator.ind_id,
                            version_id=methodology.version_id,
                            score=value_float,
                        )
                        db.add(fact)

                    values_loaded += 1

        # **Format 1**: Single sheet with all columns
        else:
            logger.info("Detected Format 1: Single sheet with all columns")
            if has_multiple_sheets:
                logger.warning(f"‚ö†Ô∏è Multiple sheets detected but not identified as multi-sheet format. Using Format 1 fallback on first sheet only.")
            df = pd.read_excel(io.BytesIO(content), sheet_name=sheet_names[0])

            for idx, row in df.iterrows():
                mo_name = None
                for col in df.columns:
                    if '–º—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç' in str(col).lower():
                        mo_name = row.get(col)
                        break

                if pd.isna(mo_name) or mo_name == '':
                    continue

                total_rows_processed += 1

                mo = db.query(DimMO).filter(
                    text("mo_name ILIKE :name")
                ).params(name=f"%{mo_name}%").first()

                if not mo:
                    continue

                for col in df.columns:
                    col_str = str(col).strip()

                    matched_code = None
                    for official_code in official_indicators:
                        if col_str == official_code or col_str.lower() == official_code:
                            matched_code = official_code
                            break

                    if not matched_code:
                        continue

                    value = row.get(col)
                    if pd.isna(value) or value == '':
                        continue

                    indicator = db.query(DimIndicator).filter(
                        DimIndicator.code == matched_code
                    ).first()

                    if not indicator:
                        continue

                    try:
                        if isinstance(value, str):
                            value_clean = value.replace('%', '').replace(' ', '').replace(',', '.')
                            value_float = float(value_clean)
                        else:
                            value_float = float(value)
                    except:
                        continue

                    existing = db.query(FactIndicator).filter(
                        FactIndicator.mo_id == mo.mo_id,
                        FactIndicator.period_id == period.period_id,
                        FactIndicator.ind_id == indicator.ind_id,
                        FactIndicator.version_id == methodology.version_id
                    ).first()

                    if existing:
                        existing.score = value_float
                    else:
                        fact = FactIndicator(
                            mo_id=mo.mo_id,
                            period_id=period.period_id,
                            ind_id=indicator.ind_id,
                            version_id=methodology.version_id,
                            score=value_float,
                        )
                        db.add(fact)

                    values_loaded += 1

        db.commit()
        logger.info(f"Loaded {values_loaded} indicator values from {total_rows_processed} rows")

        # Automatically calculate aggregated scores
        logger.info(f"Auto-calculating aggregated scores for period {period.period_id}...")
        try:
            from migrations import calculate_fact_summary_from_indicators
            calculate_fact_summary_from_indicators()
            logger.info("Aggregated scores calculated successfully")
        except Exception as e:
            logger.error(f"Error calculating aggregated scores: {e}")

        return {
            "status": "success",
            "message": "Official methodology data imported from Excel successfully!",
            "statistics": {
                "sheets_processed": len([s for s in sheet_names if s in sheet_to_code]) if not has_municipality_col else 1,
                "rows_processed": total_rows_processed,
                "values_loaded": values_loaded,
                "period": period_month,
                "period_id": period.period_id,
                "methodology": "Official 16 criteria",
                "total_sheets": len(sheet_names)
            },
            "next_steps": [
                "1. Hard refresh Rating tab (Ctrl+F5)",
                "2. Scores should display with proper aggregation"
            ]
        }

    except Exception as e:
        logger.error(f"Error importing official methodology Excel: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error importing Excel: {str(e)}")


@router.get("/official-methodology-template")
async def get_official_methodology_template(db: Session = Depends(get_db)):
    """
    Get CSV template for official methodology data import.

    Returns a CSV with municipality names and official indicator column headers.
    """
    try:
        # Get all municipalities
        municipalities = db.query(DimMO).all()

        # Official indicator codes
        official_indicators = [
            'pub_1', 'pub_2', 'pub_3', 'pub_4', 'pub_5', 'pub_6', 'pub_7', 'pub_8', 'pub_9',
            'closed_1', 'closed_2', 'closed_3', 'closed_4', 'closed_5', 'closed_6', 'closed_7', 'closed_8',
            'pen_1', 'pen_2', 'pen_3'
        ]

        # Build CSV header
        header = "–ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç," + ",".join(official_indicators)

        # Build CSV rows with municipality names
        rows = [header]
        for mo in municipalities:
            rows.append(mo.mo_name + "," + ",".join([""] * len(official_indicators)))

        csv_content = "\n".join(rows)

        return {
            "status": "success",
            "content": csv_content,
            "instructions": {
                "format": "CSV with UTF-8 encoding",
                "columns": official_indicators,
                "example": "–õ–∏–ø–µ—Ü–∫,3,5,3,3,3,3,3,3,3,6,4,5,5,3,3,6,2,-3,-3,-5",
                "note": "All 20 municipalities are pre-filled. Just add scores."
            }
        }

    except Exception as e:
        logger.error(f"Error generating template: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")


@router.post("/csv")
async def import_csv(
    file: UploadFile = File(...),
    period_month: str = "2024-01",  # –§–æ—Ä–º–∞—Ç: YYYY-MM
    db: Session = Depends(get_db),
):
    """
    Import CSV file with indicator data.
    Expected format: CSV with columns including '–ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç' and various indicators.

    Parameters:
    - file: CSV file to upload
    - period_month: Period in YYYY-MM format (default: 2024-01)
    """
    try:
        # Read CSV file
        content = await file.read()
        df = pd.read_csv(io.BytesIO(content), encoding='utf-8')

        logger.info(f"CSV uploaded: {len(df)} rows, {len(df.columns)} columns")
        logger.info(f"Target period: {period_month}")

        # Parse period_month
        from datetime import datetime, timedelta
        period_date = datetime.strptime(period_month, "%Y-%m")

        # Calculate last day of month
        if period_date.month == 12:
            period_end = period_date.replace(day=31)
        else:
            next_month = period_date.replace(month=period_date.month + 1, day=1)
            period_end = next_month - timedelta(days=1)

        date_from = period_date.strftime("%Y-%m-%d")
        date_to = period_end.strftime("%Y-%m-%d")

        # Create or get methodology v1
        methodology = db.query(DimMethodology).filter(
            DimMethodology.version == "v1"
        ).first()

        if not methodology:
            methodology = DimMethodology(
                version="v1",
                valid_from="2024-01-01",
                notes="–û—Ü–µ–Ω–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è v1"
            )
            db.add(methodology)
            db.commit()
            db.refresh(methodology)
            logger.info(f"Created methodology v1 (ID: {methodology.version_id})")

        # Create or get period
        period = db.query(DimPeriod).filter(
            DimPeriod.date_from == date_from
        ).first()

        if not period:
            period = DimPeriod(
                period_type="month",
                date_from=date_from,
                date_to=date_to,
                edg_flag=False
            )
            db.add(period)
            db.commit()
            db.refresh(period)
            logger.info(f"Created period {period_month} (ID: {period.period_id})")
        else:
            logger.info(f"Using existing period {period_month} (ID: {period.period_id})")

        # Create municipalities
        municipalities = df[['–ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç']].drop_duplicates()
        mo_created = 0

        for _, row in municipalities.iterrows():
            mo_name = row['–ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç']
            if pd.isna(mo_name) or mo_name == '':
                continue

            existing = db.query(DimMO).filter(DimMO.mo_name == mo_name).first()
            if not existing:
                mo = DimMO(
                    mo_name=mo_name,
                    type="–ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–π —Ä–∞–π–æ–Ω"
                )
                db.add(mo)
                mo_created += 1

        db.commit()
        logger.info(f"Created {mo_created} municipalities")

        # Create indicators
        skip_columns = ['–õ–∏—Å—Ç', '–ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç', '–ì–ª–∞–≤–∞ –ú–û']
        indicator_columns = [col for col in df.columns if col not in skip_columns]

        ind_created = 0
        for col_name in indicator_columns:
            code = col_name[:50].replace(' ', '_').replace(',', '').replace('(', '').replace(')', '')

            existing = db.query(DimIndicator).filter(DimIndicator.code == code).first()
            if not existing:
                indicator = DimIndicator(
                    code=code,
                    name=col_name,
                    block="–û—Ü–µ–Ω–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è",
                    is_public=True
                )
                db.add(indicator)
                ind_created += 1

        db.commit()
        logger.info(f"Created {ind_created} indicators")

        # Load indicator values
        mo_map = {mo.mo_name: mo.mo_id for mo in db.query(DimMO).all()}
        ind_map = {ind.name: ind.ind_id for ind in db.query(DimIndicator).all()}

        values_loaded = 0
        errors = 0

        for _, row in df.iterrows():
            mo_name = row['–ú—É–Ω–∏—Ü–∏–ø–∞–ª–∏—Ç–µ—Ç']
            if pd.isna(mo_name) or mo_name == '' or mo_name not in mo_map:
                continue

            mo_id = mo_map[mo_name]

            for col_name in indicator_columns:
                value = row[col_name]
                if pd.isna(value) or value == '':
                    continue

                if col_name not in ind_map:
                    continue

                ind_id = ind_map[col_name]

                # Convert value to float
                try:
                    if isinstance(value, str):
                        value_clean = value.replace('%', '').replace(' ', '').replace(',', '.')
                        try:
                            value_raw = float(value_clean)
                        except:
                            value_raw = 1.0
                    else:
                        value_raw = float(value)
                except:
                    value_raw = 1.0

                # Check if exists
                existing = db.query(FactIndicator).filter(
                    FactIndicator.mo_id == mo_id,
                    FactIndicator.period_id == period.period_id,
                    FactIndicator.ind_id == ind_id,
                    FactIndicator.version_id == methodology.version_id
                ).first()

                if existing:
                    existing.value_raw = value_raw
                    existing.score = value_raw  # –¢–∞–∫–∂–µ –æ–±–Ω–æ–≤–∏—Ç—å score
                else:
                    fact = FactIndicator(
                        mo_id=mo_id,
                        period_id=period.period_id,
                        ind_id=ind_id,
                        version_id=methodology.version_id,
                        value_raw=value_raw,
                        score=value_raw  # ‚Üê –î–û–ë–ê–í–ò–¢–¨: –∑–∞–ø–æ–ª–Ω–∏—Ç—å score
                    )
                    db.add(fact)

                values_loaded += 1

        db.commit()
        logger.info(f"Loaded {values_loaded} indicator values")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –±–∞–ª–ª—ã –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        logger.info(f"Auto-calculating scores for period {period.period_id}...")
        try:
            score_result = await calculate_summary_scores(period_id=period.period_id, db=db)
            logger.info(f"Scores calculated: {score_result}")
        except Exception as e:
            logger.error(f"Error calculating scores: {e}")
            # –ù–µ –ø–∞–¥–∞–µ–º, –µ—Å–ª–∏ –ø–µ—Ä–µ—Å—á–µ—Ç –Ω–µ —É–¥–∞–ª—Å—è - –¥–∞–Ω–Ω—ã–µ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã

        return {
            "status": "success",
            "message": "CSV imported successfully and scores calculated",
            "statistics": {
                "rows": len(df),
                "columns": len(df.columns),
                "municipalities_created": mo_created,
                "indicators_created": ind_created,
                "values_loaded": values_loaded
            },
            "scores_calculated": True
        }

    except Exception as e:
        logger.error(f"Error importing CSV: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error importing CSV: {str(e)}")


@router.get("/stats")
async def get_import_stats(db: Session = Depends(get_db)):
    """Get current database statistics"""
    try:
        stats = {
            "municipalities": db.query(DimMO).count(),
            "indicators": db.query(DimIndicator).count(),
            "periods": db.query(DimPeriod).count(),
            "methodologies": db.query(DimMethodology).count(),
            "indicator_values": db.query(FactIndicator).count(),
        }

        return {
            "status": "success",
            "statistics": stats
        }

    except Exception as e:
        logger.error(f"Error fetching stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fetching stats: {str(e)}")


@router.post("/update-coordinates")
async def update_municipality_coordinates(db: Session = Depends(get_db)):
    """
    Update coordinates for Lipetsk Oblast municipalities.
    Adds lat/lon for map visualization.
    """
    try:
        # Coordinates for Lipetsk Oblast municipalities (approximate centers)
        coordinates = {
            "–õ–∏–ø–µ—Ü–∫": (52.6031, 39.5708),
            "–ï–ª–µ—Ü": (52.6236, 38.5019),
            "–í–æ–ª–æ–≤—Å–∫–∏–π": (51.1528, 38.4500),
            "–ì—Ä—è–∑—è–Ω—Å–∫–∏–π": (52.5000, 39.9500),
            "–î–∞–Ω–∫–æ–≤—Å–∫–∏–π": (53.2500, 39.1500),
            "–î–æ–±—Ä–∏–Ω—Å–∫–∏–π": (52.2000, 40.4000),
            "–î–æ–±—Ä–æ–≤—Å–∫–∏–π": (52.3500, 39.2000),
            "–î–æ–ª–≥–æ—Ä—É–∫–æ–≤—Å–∫–∏–π": (52.3500, 38.3000),
            "–ï–ª–µ—Ü–∫–∏–π": (52.6236, 38.5019),
            "–ó–∞–¥–æ–Ω—Å–∫–∏–π": (52.4000, 38.9000),
            "–ò–∑–º–∞–ª–∫–æ–≤—Å–∫–∏–π": (52.5000, 38.8000),
            "–ö—Ä–∞—Å–Ω–∏–Ω—Å–∫–∏–π": (53.0000, 39.5000),
            "–õ–µ–±–µ–¥—è–Ω—Å–∫–∏–π": (53.0200, 39.1300),
            "–õ–µ–≤-–¢–æ–ª—Å—Ç–æ–≤—Å–∫–∏–π": (52.1000, 39.5000),
            "–õ–∏–ø–µ—Ü–∫–∏–π": (52.6031, 39.5708),
            "–°—Ç–∞–Ω–æ–≤–ª—è–Ω—Å–∫–∏–π": (53.0000, 38.5000),
            "–¢–µ—Ä–±—É–Ω—Å–∫–∏–π": (52.1500, 38.2500),
            "–£—Å–º–∞–Ω—Å–∫–∏–π": (52.0500, 39.7400),
            "–•–ª–µ–≤–µ–Ω—Å–∫–∏–π": (52.4500, 38.1000),
            "–ß–∞–ø–ª—ã–≥–∏–Ω—Å–∫–∏–π": (53.2500, 39.9500),
        }

        updated = 0
        for mo_name, (lat, lon) in coordinates.items():
            mo = db.query(DimMO).filter(DimMO.mo_name == mo_name).first()
            if mo:
                mo.lat = lat
                mo.lon = lon
                updated += 1

        db.commit()
        logger.info(f"Updated coordinates for {updated} municipalities")

        return {
            "status": "success",
            "message": f"Updated coordinates for {updated} municipalities",
            "updated": updated
        }

    except Exception as e:
        logger.error(f"Error updating coordinates: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error updating coordinates: {str(e)}")


@router.get("/run-migration")
async def run_migration_page(db: Session = Depends(get_db)):
    """
    Execute full migration: add geojson column and populate data.
    Returns HTML page with results.
    """
    from fastapi.responses import HTMLResponse

    results = []

    # Step 1: Add geojson column
    try:
        result = db.execute(text("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name='dim_mo' AND column_name='geojson'
        """))

        if result.fetchone() is None:
            db.execute(text("ALTER TABLE dim_mo ADD COLUMN geojson JSON"))
            db.commit()
            results.append(("‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è", "–ö–æ–ª–æ–Ω–∫–∞ geojson —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞"))
        else:
            results.append(("‚ÑπÔ∏è –ú–∏–≥—Ä–∞—Ü–∏—è", "–ö–æ–ª–æ–Ω–∫–∞ geojson —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"))
    except Exception as e:
        results.append(("‚ùå –ú–∏–≥—Ä–∞—Ü–∏—è", f"–û—à–∏–±–∫–∞: {str(e)}"))
        db.rollback()

    # Step 2: Update geojson data with realistic boundaries
    try:
        import math
        municipalities = db.query(DimMO).all()
        updated = 0

        for mo in municipalities:
            if mo.lat and mo.lon:
                lat, lon = mo.lat, mo.lon

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä (–≥–æ—Ä–æ–¥–∞ –º–µ–Ω—å—à–µ, —Ä–∞–π–æ–Ω—ã –±–æ–ª—å—à–µ)
                if mo.mo_name in ["–õ–∏–ø–µ—Ü–∫", "–ï–ª–µ—Ü"]:
                    size = 0.12
                else:
                    size = 0.20

                # –°–æ–∑–¥–∞–µ–º —ç–ª–ª–∏–ø—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–ª–∏–≥–æ–Ω —Å –≤–æ–ª–Ω–∏—Å—Ç—ã–º–∏ –∫—Ä–∞—è–º–∏
                points_count = 48
                coordinates = []

                for i in range(points_count):
                    angle = (i * 360 / points_count) * (math.pi / 180)

                    # –≠–ª–ª–∏–ø—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞
                    a = size * 1.5
                    b = size

                    # –í–æ–ª–Ω–∏—Å—Ç–æ—Å—Ç—å
                    wave = 1.0 + 0.15 * math.sin(5 * angle) + 0.1 * math.cos(7 * angle)

                    r = (a * b) / math.sqrt((b * math.cos(angle))**2 + (a * math.sin(angle))**2)
                    r *= wave

                    point_lat = lat + r * math.sin(angle)
                    point_lon = lon + r * math.cos(angle) / math.cos(lat * math.pi / 180)

                    coordinates.append([point_lon, point_lat])

                # –ó–∞–º—ã–∫–∞–µ–º –ø–æ–ª–∏–≥–æ–Ω
                coordinates.append(coordinates[0])

                geojson_data = {
                    "type": "Polygon",
                    "coordinates": [coordinates]
                }

                mo.geojson = geojson_data
                updated += 1

        db.commit()
        results.append(("‚úÖ GeoJSON", f"–û–±–Ω–æ–≤–ª–µ–Ω–æ –≥—Ä–∞–Ω–∏—Ü: {updated} (—ç–ª–ª–∏–ø—Ç–∏—á–µ—Å–∫–∏–µ —Å –≤–æ–ª–Ω–∏—Å—Ç—ã–º–∏ –∫—Ä–∞—è–º–∏)"))
    except Exception as e:
        results.append(("‚ùå GeoJSON", f"–û—à–∏–±–∫–∞: {str(e)}"))
        db.rollback()

    # Generate HTML response
    html_content = """
    <!DOCTYPE html>
    <html lang="ru">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–∏–≥—Ä–∞—Ü–∏–∏</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                max-width: 800px;
                margin: 50px auto;
                padding: 20px;
                background: #f5f5f5;
            }
            .container {
                background: white;
                padding: 30px;
                border-radius: 8px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
            h1 { color: #333; }
            .result {
                margin: 15px 0;
                padding: 15px;
                border-left: 4px solid #3b82f6;
                background: #f0f9ff;
            }
            .result h3 {
                margin: 0 0 10px 0;
                color: #1e40af;
            }
            .result p {
                margin: 0;
                color: #475569;
            }
            .success { border-left-color: #10b981; background: #d1fae5; }
            .success h3 { color: #065f46; }
            .error { border-left-color: #ef4444; background: #fee2e2; }
            .error h3 { color: #991b1b; }
            .button {
                display: inline-block;
                margin-top: 20px;
                padding: 12px 24px;
                background: #3b82f6;
                color: white;
                text-decoration: none;
                border-radius: 4px;
            }
            .button:hover {
                background: #2563eb;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üîß –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–∏–≥—Ä–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö</h1>
    """

    for title, message in results:
        status_class = "success" if "‚úÖ" in title or "‚ÑπÔ∏è" in title else "error"
        html_content += f"""
            <div class="result {status_class}">
                <h3>{title}</h3>
                <p>{message}</p>
            </div>
        """

    html_content += """
            <a href="/map" class="button">–û—Ç–∫—Ä—ã—Ç—å –∫–∞—Ä—Ç—É</a>
            <a href="/api/docs" class="button" style="background: #6b7280;">API Docs</a>
        </div>
    </body>
    </html>
    """

    return HTMLResponse(content=html_content)


@router.post("/migrate-add-geojson")
async def migrate_add_geojson_column(db: Session = Depends(get_db)):
    """
    Add geojson column to dim_mo table if it doesn't exist.
    This is a one-time migration.
    """
    try:
        # Check if column exists
        result = db.execute(text("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name='dim_mo' AND column_name='geojson'
        """))

        if result.fetchone() is None:
            # Add column
            db.execute(text("ALTER TABLE dim_mo ADD COLUMN geojson JSON"))
            db.commit()
            logger.info("Added geojson column to dim_mo table")
            return {
                "status": "success",
                "message": "geojson column added to dim_mo table"
            }
        else:
            return {
                "status": "success",
                "message": "geojson column already exists"
            }

    except Exception as e:
        logger.error(f"Error adding geojson column: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error adding geojson column: {str(e)}")


@router.post("/upload-real-boundaries")
async def upload_real_boundaries(
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    Upload real GeoJSON boundaries from file.
    Expects standard GeoJSON FeatureCollection with properties.name for each municipality.

    Format:
    {
      "type": "FeatureCollection",
      "features": [
        {
          "type": "Feature",
          "properties": {"name": "–õ–∏–ø–µ—Ü–∫", ...},
          "geometry": {"type": "Polygon", "coordinates": [...]}
        },
        ...
      ]
    }
    """
    try:
        # Read uploaded file
        content = await file.read()
        data = json.loads(content.decode('utf-8'))

        logger.info(f"Uploading real boundaries from file: {file.filename}")

        # Support both FeatureCollection and old format
        if data.get("type") == "FeatureCollection":
            features = data.get("features", [])
            logger.info(f"Found FeatureCollection with {len(features)} features")

            updated = 0
            not_found = []
            errors = []

            for feature in features:
                if feature.get("type") != "Feature":
                    errors.append(f"Invalid feature type: {feature.get('type')}")
                    continue

                properties = feature.get("properties", {})
                geometry = feature.get("geometry")

                # Get municipality name from properties
                mo_name = properties.get("name") or properties.get("NAME") or properties.get("mo_name")

                if not mo_name:
                    errors.append("Feature missing 'name' property")
                    continue

                mo_name = mo_name.strip()

                if not geometry:
                    errors.append(f"Feature {mo_name} missing geometry")
                    continue

                # Find municipality in database (case-insensitive)
                mo = db.query(DimMO).filter(
                    DimMO.mo_name.ilike(mo_name)
                ).first()

                if mo:
                    # Store as GeoJSON Feature
                    mo.geojson = {
                        "type": "Feature",
                        "properties": {"name": mo.mo_name},  # Use DB name for consistency
                        "geometry": geometry
                    }
                    updated += 1
                    logger.info(f"Updated {mo.mo_name}")
                else:
                    not_found.append(mo_name)
                    logger.warning(f"Municipality not found in DB: {mo_name}")

            db.commit()

            return {
                "status": "success",
                "message": f"Uploaded real boundaries for {updated} municipalities",
                "updated": updated,
                "not_found": not_found,
                "errors": errors,
                "note": "Real boundaries from GeoJSON FeatureCollection"
            }

        else:
            # Old format: {"–ú–û_name": {"type": "Polygon", "coordinates": [...]}}
            logger.info(f"Found {len(data)} municipalities in old format")

            updated = 0
            not_found = []

            for mo_name, geojson_data in data.items():
                mo = db.query(DimMO).filter(DimMO.mo_name == mo_name).first()

                if mo:
                    mo.geojson = geojson_data
                    updated += 1
                    logger.info(f"Updated {mo_name}")
                else:
                    not_found.append(mo_name)
                    logger.warning(f"Municipality not found in DB: {mo_name}")

            db.commit()

            return {
                "status": "success",
                "message": f"Uploaded real boundaries for {updated} municipalities",
                "updated": updated,
                "not_found": not_found,
                "note": "Real boundaries from legacy format"
            }

    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Invalid JSON file: {str(e)}")
    except Exception as e:
        logger.error(f"Error uploading boundaries: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error uploading boundaries: {str(e)}")


@router.post("/update-geojson-realistic")
async def update_municipality_geojson_realistic(db: Session = Depends(get_db)):
    """
    Update GeoJSON boundaries with realistic elliptical shapes.
    Creates more natural-looking boundaries based on municipality centers.
    """
    import math

    try:
        municipalities = db.query(DimMO).all()
        updated = 0

        for mo in municipalities:
            if mo.lat and mo.lon:
                lat, lon = mo.lat, mo.lon

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–≥–æ—Ä–æ–¥–∞ –º–µ–Ω—å—à–µ, —Ä–∞–π–æ–Ω—ã –±–æ–ª—å—à–µ)
                if mo.mo_name in ["–õ–∏–ø–µ—Ü–∫", "–ï–ª–µ—Ü"]:
                    size = 0.12
                else:
                    size = 0.20

                # –°–æ–∑–¥–∞–µ–º —ç–ª–ª–∏–ø—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–ª–∏–≥–æ–Ω —Å –≤–æ–ª–Ω–∏—Å—Ç—ã–º–∏ –∫—Ä–∞—è–º–∏
                points = 48
                coordinates = []

                for i in range(points):
                    angle = (i * 360 / points) * (math.pi / 180)

                    # –≠–ª–ª–∏–ø—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞
                    a = size * 1.5  # –ë–æ–ª—å—à–∞—è –ø–æ–ª—É–æ—Å—å
                    b = size  # –ú–∞–ª–∞—è –ø–æ–ª—É–æ—Å—å

                    # –î–æ–±–∞–≤–ª—è–µ–º –≤–æ–ª–Ω–∏—Å—Ç–æ—Å—Ç—å
                    wave = 1.0 + 0.15 * math.sin(5 * angle) + 0.1 * math.cos(7 * angle)

                    r = (a * b) / math.sqrt((b * math.cos(angle))**2 + (a * math.sin(angle))**2)
                    r *= wave

                    point_lat = lat + r * math.sin(angle)
                    point_lon = lon + r * math.cos(angle) / math.cos(lat * math.pi / 180)

                    coordinates.append([point_lon, point_lat])

                # –ó–∞–º—ã–∫–∞–µ–º –ø–æ–ª–∏–≥–æ–Ω
                coordinates.append(coordinates[0])

                geojson_data = {
                    "type": "Polygon",
                    "coordinates": [coordinates]
                }

                mo.geojson = geojson_data
                updated += 1

        db.commit()
        logger.info(f"Updated realistic GeoJSON for {updated} municipalities")

        return {
            "status": "success",
            "message": f"Updated realistic boundaries for {updated} municipalities",
            "updated": updated,
            "note": "Using elliptical shapes with wavy edges for natural appearance"
        }

    except Exception as e:
        logger.error(f"Error updating realistic GeoJSON: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error updating realistic GeoJSON: {str(e)}")


@router.post("/update-geojson")
async def update_municipality_geojson(db: Session = Depends(get_db)):
    """
    Update GeoJSON boundaries for Lipetsk Oblast municipalities.
    For now, this generates simplified polygon boundaries based on coordinates.
    In production, this should be replaced with real GeoJSON data from OpenStreetMap or similar.
    """
    try:
        municipalities = db.query(DimMO).all()
        updated = 0

        for mo in municipalities:
            if mo.lat and mo.lon:
                # Generate a more realistic polygon (hexagon instead of rectangle)
                # This is still simplified - real boundaries should come from OSM
                size = 0.15
                lat, lon = mo.lat, mo.lon

                # Create a hexagon-like shape
                import math
                points = []
                for i in range(6):
                    angle = (i * 60) * (math.pi / 180)
                    point_lon = lon + size * math.cos(angle)
                    point_lat = lat + size * math.sin(angle)
                    points.append([point_lon, point_lat])

                # Close the polygon
                points.append(points[0])

                # GeoJSON Polygon format
                geojson_data = {
                    "type": "Polygon",
                    "coordinates": [points]
                }

                mo.geojson = geojson_data
                updated += 1

        db.commit()
        logger.info(f"Updated GeoJSON for {updated} municipalities")

        return {
            "status": "success",
            "message": f"Updated GeoJSON for {updated} municipalities",
            "updated": updated,
            "note": "Using generated hexagon boundaries. Replace with real OSM data for production."
        }

    except Exception as e:
        logger.error(f"Error updating GeoJSON: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error updating GeoJSON: {str(e)}")


@router.post("/calculate-scores")
async def calculate_summary_scores(
    period_id: int = None,
    db: Session = Depends(get_db)
):
    """
    Calculate summary scores for all municipalities.
    Creates records in fact_summary table based on indicator values.

    Parameters:
    - period_id: Optional. If not provided, calculates for all periods.
    """
    try:
        from models import FactSummary

        # Get all municipalities
        municipalities = db.query(DimMO).all()

        # Get periods
        if period_id:
            periods = [db.query(DimPeriod).filter(DimPeriod.period_id == period_id).first()]
            if not periods[0]:
                raise HTTPException(status_code=404, detail=f"Period {period_id} not found")
        else:
            # Calculate for all periods
            periods = db.query(DimPeriod).all()

        methodology = db.query(DimMethodology).filter(DimMethodology.version == "v1").first()

        if not periods or not methodology:
            raise HTTPException(status_code=404, detail="Period or methodology not found")

        total_created = 0
        total_updated = 0

        # Process each period
        for period in periods:
            created = 0
            updated = 0

            for mo in municipalities:
                # Get all indicator values for this MO and period
                indicator_values = db.query(FactIndicator).filter(
                    FactIndicator.mo_id == mo.mo_id,
                    FactIndicator.period_id == period.period_id,
                    FactIndicator.version_id == methodology.version_id
                ).all()

                if not indicator_values:
                    continue

                # Calculate simple average score (–º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É)
                total_values = sum([v.value_raw for v in indicator_values if v.value_raw])
                count_values = len([v for v in indicator_values if v.value_raw])

                if count_values > 0:
                    avg_score = total_values / count_values
                    # Normalize to 0-100 scale
                    score_total = min(100, max(0, avg_score))

                    # Determine zone based on score
                    if score_total >= 70:
                        zone = "green"
                    elif score_total >= 40:
                        zone = "yellow"
                    else:
                        zone = "red"
                else:
                    score_total = 0
                    zone = "red"

                # Check if summary exists
                existing = db.query(FactSummary).filter(
                    FactSummary.mo_id == mo.mo_id,
                    FactSummary.period_id == period.period_id,
                    FactSummary.version_id == methodology.version_id
                ).first()

                if existing:
                    existing.score_total = score_total
                    existing.score_public = score_total
                    existing.zone = zone
                    updated += 1
                else:
                    summary = FactSummary(
                        mo_id=mo.mo_id,
                        period_id=period.period_id,
                        version_id=methodology.version_id,
                        score_public=score_total,
                        score_total=score_total,
                        zone=zone
                    )
                    db.add(summary)
                    created += 1

            total_created += created
            total_updated += updated
            logger.info(f"Period {period.period_id}: Created {created}, updated {updated}")

        db.commit()
        logger.info(f"Total: Created {total_created} summaries, updated {total_updated}")

        return {
            "status": "success",
            "message": f"Calculated scores for {len(periods)} period(s)",
            "periods_processed": len(periods),
            "created": total_created,
            "updated": total_updated
        }

    except Exception as e:
        logger.error(f"Error calculating scores: {str(e)}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Error calculating scores: {str(e)}")
